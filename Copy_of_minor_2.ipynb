{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUzg5TZV5xZErxJkQ4RKEI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madarasanj/minor-2/blob/main/Copy_of_minor_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_94iYAtKaC",
        "outputId": "ade7e1bb-5cbc-4718-cbce-3f70ba645fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m51.2/62.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.25.2)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56441 sha256=04725cf2d7e6022dc818403600e78e7873c889976b743e4a4bd2e2a3d2686f8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn4bZ_1Itwb3",
        "outputId": "3ae818a0-e0a6-4ee4-b783-a6a1f150a9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FxQEBsKu5lt",
        "outputId": "ef8e716a-da4a-483d-c4b0-c035c83e1a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpGdr-USsAQd",
        "outputId": "b32c4987-7ace-4463-8247-1df5dced1e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "127/127 [==============================] - 13s 80ms/step - loss: 0.5998 - accuracy: 0.7128 - val_loss: 0.5963 - val_accuracy: 0.7054\n",
            "Epoch 2/15\n",
            "127/127 [==============================] - 10s 81ms/step - loss: 0.5776 - accuracy: 0.7150 - val_loss: 0.5842 - val_accuracy: 0.7054\n",
            "Epoch 3/15\n",
            "127/127 [==============================] - 9s 72ms/step - loss: 0.5484 - accuracy: 0.7361 - val_loss: 0.5892 - val_accuracy: 0.7153\n",
            "Epoch 4/15\n",
            "127/127 [==============================] - 10s 76ms/step - loss: 0.5087 - accuracy: 0.7588 - val_loss: 0.6122 - val_accuracy: 0.7054\n",
            "Epoch 5/15\n",
            "127/127 [==============================] - 10s 76ms/step - loss: 0.4609 - accuracy: 0.7934 - val_loss: 0.6299 - val_accuracy: 0.6416\n",
            "Epoch 6/15\n",
            "127/127 [==============================] - 8s 67ms/step - loss: 0.4062 - accuracy: 0.8248 - val_loss: 0.7642 - val_accuracy: 0.6431\n",
            "Epoch 7/15\n",
            "127/127 [==============================] - 11s 87ms/step - loss: 0.3623 - accuracy: 0.8484 - val_loss: 0.8055 - val_accuracy: 0.6686\n",
            "Epoch 8/15\n",
            "127/127 [==============================] - 10s 76ms/step - loss: 0.3297 - accuracy: 0.8621 - val_loss: 0.6825 - val_accuracy: 0.6586\n",
            "Epoch 9/15\n",
            "127/127 [==============================] - 10s 76ms/step - loss: 0.2938 - accuracy: 0.8818 - val_loss: 0.8606 - val_accuracy: 0.6445\n",
            "Epoch 10/15\n",
            "127/127 [==============================] - 9s 67ms/step - loss: 0.2666 - accuracy: 0.8949 - val_loss: 0.9532 - val_accuracy: 0.6190\n",
            "Epoch 11/15\n",
            "127/127 [==============================] - 10s 76ms/step - loss: 0.2442 - accuracy: 0.9047 - val_loss: 0.8547 - val_accuracy: 0.6700\n",
            "Epoch 12/15\n",
            "127/127 [==============================] - 10s 77ms/step - loss: 0.2194 - accuracy: 0.9130 - val_loss: 0.9626 - val_accuracy: 0.6671\n",
            "Epoch 13/15\n",
            "127/127 [==============================] - 9s 74ms/step - loss: 0.2036 - accuracy: 0.9219 - val_loss: 1.1334 - val_accuracy: 0.6657\n",
            "Epoch 14/15\n",
            "127/127 [==============================] - 9s 68ms/step - loss: 0.1892 - accuracy: 0.9288 - val_loss: 1.3400 - val_accuracy: 0.6856\n",
            "Epoch 15/15\n",
            "127/127 [==============================] - 10s 76ms/step - loss: 0.1774 - accuracy: 0.9334 - val_loss: 1.4006 - val_accuracy: 0.6643\n",
            "56/56 [==============================] - 1s 15ms/step - loss: 1.3959 - accuracy: 0.6718\n",
            "Test Accuracy: 67.18%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Constants and parameters\n",
        "stop = stopwords.words('english')\n",
        "sequence_length = 20  # Adjust as per your data\n",
        "embedding_dim = 100  # Adjust as per your data\n",
        "dropout_prob = [0.1]  # Adjust as per your data\n",
        "val_split = 0.1  # Validation split ratio\n",
        "batch_size = 50  # Batch size for training\n",
        "\n",
        "# Function to preprocess text data\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \"\", string)\n",
        "    string = re.sub(r\"\\)\", \"\", string)\n",
        "    string = re.sub(r\"\\?\", \"\", string)\n",
        "    string = re.sub(r\"/\", \"\", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "# Function to remove numbers and tokenize text\n",
        "def number_removal(row):\n",
        "    data1 = row['tweet']\n",
        "    if type(data1) not in [int, float]:\n",
        "        line = re.sub(r\"[^A-Za-z\\s]\", \" \", data1.strip())\n",
        "        tokens = line.split()\n",
        "    else:\n",
        "        tokens = []\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to generate word frequency\n",
        "def generate_word_frequency(row):\n",
        "    data1 = row['tweet']\n",
        "    tokens = nltk.wordpunct_tokenize(data1)\n",
        "    token_list = []\n",
        "    for token in tokens:\n",
        "        token_list.append(token.lower())\n",
        "    return ','.join(token_list)\n",
        "\n",
        "# Function to lemmatize words\n",
        "def lemmatize_text(tokens):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens.split(',')]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Preprocessing and cleaning data\n",
        "def preprocess_data(data):\n",
        "    data['tweet'] = data.apply(number_removal, axis=1)\n",
        "    data['tokens'] = data.apply(generate_word_frequency, axis=1)\n",
        "    data['tweet_lem'] = data['tokens'].apply(lemmatize_text)\n",
        "    return data['tweet_lem']\n",
        "\n",
        "# Pad sentences to the same length\n",
        "def pad_sentences(sentences, sequence_length, padding_word=\"<PAD/>\"):\n",
        "    padded_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if len(sentence) >= sequence_length:\n",
        "            new_sentence = sentence[:sequence_length]\n",
        "        else:\n",
        "            num_padding = sequence_length - len(sentence)\n",
        "            new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(sentences):\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return vocabulary, vocabulary_inv\n",
        "\n",
        "# Convert sentences and labels to vectors based on vocabulary\n",
        "def build_input_data(sentences, labels, vocabulary):\n",
        "    x = np.array([[vocabulary[str(word)] for word in sentence] if isinstance(sentence, list) else vocabulary[str(sentence)] for sentence in sentences])\n",
        "    y = np.array(labels)\n",
        "    return x, y\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_data(sequence_length):\n",
        "    data = pd.read_csv(\"/content/bullying_dataset.csv\")\n",
        "    x_text = preprocess_data(data)\n",
        "    y = data['label'].to_numpy()\n",
        "    x_text = [clean_str(sent) for sent in x_text]\n",
        "    x_text = [s.split(\" \") for s in x_text]\n",
        "    sentences_padded = pad_sentences(x_text, sequence_length)\n",
        "    vocabulary, _ = build_vocab(sentences_padded)\n",
        "    x, y = build_input_data(sentences_padded, y, vocabulary)\n",
        "    return x, y, vocabulary\n",
        "\n",
        "# Load and preprocess the data\n",
        "sequence_length = 20  # Adjust as per your data\n",
        "x, y, vocabulary = load_data(sequence_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define and compile the model (you can choose either LSTM or CNN)\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length))\n",
        "model.add(LSTM(200, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=15, validation_split=val_split, batch_size=batch_size)\n",
        "\n",
        "# Evaluate the model\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Test Accuracy: %.2f%%\" % (scores[1] * 100))\n"
      ]
    }
  ]
}